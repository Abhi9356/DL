import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np
import os
import zipfile
import requests # A library for making HTTP requests


import os
import zipfile

# --- This part automatically gets your file ---
zip_path = "/content/cats_and_dogs_filtered.zip"
url = "https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip"

if not os.path.exists(zip_path):
    print("Downloading the dataset...")
    # Use !wget to download the file directly into Colab
    !wget --no-check-certificate {url} -O {zip_path}
    print("Download complete.")
else:
    print("Dataset already present.")
# ---

# Your original code now runs without error
print("Unzipping the dataset...")
with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall("/content")

train_dir = os.path.join("/content/cats_and_dogs_filtered", "train")
validation_dir = os.path.join("/content/cats_and_dogs_filtered", "validation")
print("Setup complete. Directories are ready.")
     

img_height, img_width = 160, 160
batch_size = 32

train_ds = tf.keras.utils.image_dataset_from_directory(
    train_dir,
    image_size=(img_height, img_width),
    batch_size=batch_size,
    shuffle=True
)

val_ds = tf.keras.utils.image_dataset_from_directory(
    validation_dir,
    image_size=(img_height, img_width),
    batch_size=batch_size
)

class_names = train_ds.class_names
print("Class Names:", class_names)

AUTOTUNE = tf.data.AUTOTUNE
train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)
val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)

base_model = tf.keras.applications.MobileNetV2(
    input_shape=(img_height, img_width, 3),
    include_top=False,        # remove the original classifier
    weights='imagenet'        # use pretrained ImageNet weights
)

base_model.trainable = False


global_average_layer = tf.keras.layers.GlobalAveragePooling2D()
prediction_layer = tf.keras.layers.Dense(1, activation='sigmoid')

model = tf.keras.Sequential([
    tf.keras.layers.Rescaling(1./255),
    base_model,
    global_average_layer,
    prediction_layer
])


model.compile(
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),
    loss='binary_crossentropy',
    metrics=['accuracy']
)

history = model.fit(
    train_ds,
    validation_data=val_ds,
    epochs=5
)
     

acc = history.history['accuracy']
val_acc = history.history['val_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']
epochs = range(1, len(acc) + 1)

plt.figure(figsize=(12,5))
plt.subplot(1,2,1)
plt.plot(epochs, acc, 'b', label='Training acc')
plt.plot(epochs, val_acc, 'r', label='Validation acc')
plt.legend()
plt.title("Accuracy")

plt.subplot(1,2,2)
plt.plot(epochs, loss, 'b', label='Training loss')
plt.plot(epochs, val_loss, 'r', label='Validation loss')
plt.legend()
plt.title("Loss")
plt.show()

for images, labels in val_ds.take(1):
    preds = model.predict(images)
    pred_labels = (preds > 0.5).astype("int32")
    plt.figure(figsize=(10,10))
    for i in range(9):
        ax = plt.subplot(3,3,i+1)
        plt.imshow(images[i].numpy().astype("uint8"))
        plt.title(f"Pred: {class_names[pred_labels[i][0]]}\nTrue: {class_names[labels[i]]}")
        plt.axis("off")